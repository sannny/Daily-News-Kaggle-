{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn-pandas\n",
    "pip install textblob\n",
    "pip install texthero\n",
    "pip install ktrain\n",
    "pip install pipelinehelper\n",
    "pip install tensorflow\n",
    "pip install xgboost\n",
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import texthero as hero\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from statistics import *\n",
    "import tensorflow as tf\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pipelinehelper import PipelineHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "#drive.mount('/content/gdrive', force_remount= True)\n",
    "print('Drive mounted')\n",
    "Link = \"https://drive.google.com/uc?export=download&id=1QziOipPF5sqkXiGYqrmrq_ut4vIXCr_n\"\n",
    "\n",
    "#file = r'/content/gdrive/My Drive/Colab Notebooks/Daily news/archive/Combined_News_DJIA.csv'\n",
    "Combined_News = pd.read_csv('Combined_News_DJIA.csv')\n",
    "#Combined_News = pd.read_csv(file)\n",
    "print(Combined_News.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Combined_News\n",
    "df = pd.melt(df, id_vars =['Label','Date'],value_vars =['Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7',\n",
    "       'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15',\n",
    "       'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23',\n",
    "       'Top24', 'Top25'], \n",
    "              var_name ='Top_#', value_name ='Headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values in every column\n",
    "print('Checking for null values in every col before')\n",
    "print(df.isnull().sum())\n",
    "#dropping null values\n",
    "df = df.dropna()\n",
    "print('Checking for null values in every col after')\n",
    "print(df.isnull().sum())\n",
    "df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning string\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "def cleaning_strings(news):\n",
    "  if news.find('b',0,1) != -1:\n",
    "    list_n = list(news)\n",
    "    list_n = list_n[1:]\n",
    "    news = ''.join(list_n)\n",
    "  t = pd.Series(news)\n",
    "  t = hero.remove_punctuation(t)\n",
    "  #t = hero.remove_stopwords(t)\n",
    "  #t = hero.remove_whitespace(t)\n",
    "  #print(t)\n",
    "  #t[0] = [stemmer.stem(w) for w in analyzer(t[0])]\n",
    "  news = ''.join(t[0])\n",
    "  #news = news[1:]\n",
    "  news = news.lower()\n",
    "  return news\n",
    "df2['Headline'] =df2['Headline'].apply(lambda x: cleaning_strings(x))\n",
    "df2['Date'] = pd.to_datetime(df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    print(type(doc))\n",
    "    #print((stemmer.stem(w) for w in analyzer(doc)))\n",
    "    doc = ' '.join([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_models_indi(X_train,y_train):\n",
    "  param_range = [9, 10]\n",
    "  param_range_fl = [1.0, 0.5]\n",
    "  pipe_nb = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('MultinomialNB',MultinomialNB()),\n",
    "  ])),\n",
    "  ])\n",
    "  params_nb = {\n",
    "    'vectorizer__selected_model': pipe_nb.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_nb.named_steps['classifier'].generate({\n",
    "        'MultinomialNB__alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    })\n",
    "    }\n",
    "  pipe_lr = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('lr',LogisticRegression(max_iter=500))\n",
    "  ])),\n",
    "  ])\n",
    "  params_lr = {\n",
    "    'vectorizer__selected_model': pipe_lr.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_lr.named_steps['classifier'].generate({\n",
    "        'lr__penalty' : ['l1','l2','elasticnet'],\n",
    "        'lr__solver' : ['liblinear'],\n",
    "    })\n",
    "  }\n",
    "  pipe_knn = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('knn',KNeighborsClassifier(algorithm = 'auto')),\n",
    "  ])),\n",
    "  ])\n",
    "  params_knn = {\n",
    "    'vectorizer__selected_model': pipe_knn.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_knn.named_steps['classifier'].generate({\n",
    "        'knn__n_neighbors' : [3,5,7,10,14,18,23],\n",
    "        'knn__weights' : ['uniform', 'distance'],\n",
    "      })\n",
    "  }\n",
    "  pipe_svc = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('svm',SVC()),\n",
    "    ])),\n",
    "  ])\n",
    "  params_svc = {\n",
    "    'vectorizer__selected_model': pipe_svc.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_svc.named_steps['classifier'].generate({\n",
    "        'svm__C': [1, 10, 100, 1000],\n",
    "        'svm__kernel': ['linear','rbf','poly','sigmoid','precomputed'],\n",
    "        'svm__gamma' : [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "    })\n",
    "  }\n",
    "  pipe_dt = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('decisiontree', DecisionTreeClassifier())\n",
    "  ])),\n",
    "  ])\n",
    "  params_dt = {\n",
    "    'vectorizer__selected_model': pipe_dt.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_dt.named_steps['classifier'].generate({\n",
    "        'decisiontree__max_depth': [40,50,70,90,120,150],\n",
    "    })\n",
    "  }\n",
    "  pipe_rf = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('rf',RandomForestClassifier()),\n",
    "  ])),\n",
    "  ])\n",
    "  params_rf = {\n",
    "    'vectorizer__selected_model': pipe_rf.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_rf.named_steps['classifier'].generate({\n",
    "        'rf__criterion': ['gini', 'entropy'],\n",
    "    })\n",
    "  }\n",
    "  pipe_xg = Pipeline([\n",
    "  ('vectorizer', PipelineHelper([\n",
    "    ('vect', CountVectorizer(analyzer=stemmed_words, stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "  ])),\n",
    "  ('classifier', PipelineHelper([\n",
    "    ('xgclassifier',XGBClassifier())\n",
    "  ])),\n",
    "  ])\n",
    "  params_xg = {\n",
    "    'vectorizer__selected_model': pipe_xg.named_steps['vectorizer'].generate({\n",
    "        'vect__max_df': [0.5, 0.75, 1.0],\n",
    "        'vect__max_features': [None, 5000, 10000, 50000],\n",
    "        'vect__ngram_range': [(1, 1), (2, 2)],  # unigrams or bigrams\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        #'std__with_mean': [True, False],\n",
    "        #'std__with_std': [True, False],\n",
    "        #'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe_xg.named_steps['classifier'].generate({\n",
    "        'xgclassifier__n_estimators': [400, 700, 1000],\n",
    "        'xgclassifier__colsample_bytree': [0.7, 0.8],\n",
    "        'xgclassifier__max_depth': [15,20,25],\n",
    "        'xgclassifier__reg_alpha': [1.1, 1.2, 1.3],\n",
    "        'xgclassifier__reg_lambda': [1.1, 1.2, 1.3],\n",
    "        'xgclassifier__subsample': [0.7, 0.8, 0.9]\n",
    "    })\n",
    "  }\n",
    "  #grid_nb = GridSearchCV(pipe_nb, params_nb, scoring='accuracy', verbose=1,cv = 5,n_jobs=-1)\n",
    "  #grid_nb.fit(X_train,y_train)\n",
    "  #print('Model name : Navies Bayes')\n",
    "  #print(grid_nb.best_params_)\n",
    "  #print(grid_nb.best_score_)\n",
    "  #grid_lr = GridSearchCV(pipe_lr, params_lr, scoring='accuracy', verbose=1,cv = 5,n_jobs=-1)\n",
    "  #grid_lr.fit(X_train,y_train)\n",
    "  #print('Model name : logistic regression')\n",
    "  #print(grid_lr.best_params_)\n",
    "  #print(grid_lr.best_score_)\n",
    "  grid_knn = GridSearchCV(pipe_knn, params_knn, scoring='accuracy',cv = 5,n_jobs=-1)\n",
    "  grid_knn.fit(X_train,y_train)\n",
    "  print('Model name : Knn')\n",
    "  print(grid_knn.best_params_)\n",
    "  print(grid_knn.best_score_)\n",
    "  grid_svc = GridSearchCV(pipe_svc, params_svc, scoring='accuracy',cv = 5,n_jobs=-1)\n",
    "  grid_svc.fit(X_train,y_train)\n",
    "  print('Model name : SVM')\n",
    "  print(grid_svc.best_params_)\n",
    "  print(grid_svc.best_score_)\n",
    "  grid_rf = GridSearchCV(pipe_rf, params_rf, scoring='accuracy',cv = 5,n_jobs=-1)\n",
    "  grid_rf.fit(X_train,y_train)\n",
    "  print('Model name : random forest')\n",
    "  print(grid_rf.best_params_)\n",
    "  print(grid_rf.best_score_)\n",
    "  grid_dt = GridSearchCV(pipe_dt, params_dt, scoring='accuracy',cv = 5,n_jobs=-1)\n",
    "  grid_dt.fit(X_train,y_train)\n",
    "  print('Model name : Decision Tree')\n",
    "  print(grid_dt.best_params_)\n",
    "  print(grid_dt.best_score_)\n",
    "  grid_xg = GridSearchCV(pipe_xg, params_xg, scoring='accuracy',cv = 5,n_jobs=-1)\n",
    "  grid_xg.fit(X_train,y_train)\n",
    "  print('Model name : XGBoost')\n",
    "  print(grid_xg.best_params_)\n",
    "  print(grid_xg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and testing datasets\n",
    "train = df2[df2['Date'] <= pd.to_datetime('2015-01-01').date()]\n",
    "test = df2[df2['Date'] >= pd.to_datetime('2014-12-31').date()]\n",
    "train.head(5)\n",
    "y_train = train['Label']\n",
    "X_train = train.drop(['Label'], axis=1)\n",
    "X_train = X_train['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models_indi(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
